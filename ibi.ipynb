{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intelligence Bio Inspiree\n",
    "## TP : Reseaux de neurones\n",
    "\n",
    "#### import of libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import gzip # pour décompresser les données\n",
    "import pickle # pour désérialiser les données\n",
    "import numpy # pour pouvoir utiliser des matrices\n",
    "import matplotlib.pyplot as plt # pour l'affichage\n",
    "import torch,torch.utils.data\n",
    "from numpy import array, dot\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and read train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 1\n",
    "# on charge les données de la base MNIST\n",
    "data = pickle.load(gzip.open('mnist.pkl.gz'),encoding='bytes')\n",
    "# images de la base d'apprentissage\n",
    "train_data = torch.Tensor(data[0][0])\n",
    "# labels de la base d'apprentissage\n",
    "train_data_label = torch.Tensor(data[0][1])\n",
    "# images de la base de test\n",
    "test_data = torch.Tensor(data[1][0])\n",
    "# labels de la base de test\n",
    "test_data_label = torch.Tensor(data[1][1])\n",
    "# on crée la base de données d'apprentissage (pour torch)\n",
    "train_dataset = torch.utils.data.TensorDataset(train_data,train_data_label)\n",
    "# on crée la base de données de test (pour torch)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_data,test_data_label)\n",
    "# on crée le lecteur de la base de données d'apprentissage (pour torch)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
    "# on crée le lecteur de la base de données de test (pour torch)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=1, shuffle=False)\n",
    "# 10 fois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Les differentes fonctions utilisées "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fonction qui sert à initialiser les poids au debut de l'apprentissage\n",
    "\n",
    "def Weights(num,LowerBound, UpperBound):\n",
    "    w=[]\n",
    "    for i in range(num): \n",
    "        w.append(random.uniform(LowerBound, UpperBound))\n",
    "    \n",
    "    return w \n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return float(1 / (1 + math.exp(-x)))\n",
    "\n",
    "\n",
    "def affichage(image,label):\n",
    "    # on redimensionne l'image en 28x28\n",
    "    image = image.reshape(28,28)\n",
    "    # on récupère à quel chiffre cela correspond (position du 1 dans label)\n",
    "    label = numpy.argmax(label)\n",
    "    # on crée une figure\n",
    "    plt.figure()\n",
    "    # affichage du chiffre\n",
    "    # le paramètre interpolation='nearest' force python à afficher chaque valeur de la matrice sans l'interpoler avec ses voisines\n",
    "    # le paramètre cmap définit l'échelle de couleur utilisée (ici noire et blanc)\n",
    "    plt.imshow(image,interpolation='nearest',cmap='binary')\n",
    "    # on met un titre\n",
    "    plt.title('chiffre '+str(label))\n",
    "    # on affichage les figures créées\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def affichage2(image,label):\n",
    "    # on redimensionne l'image en 28x28\n",
    "    image = image.reshape(28,28)\n",
    "    # on récupère à quel chiffre cela correspond (position du 1 dans label)\n",
    "    # on crée une figure\n",
    "    plt.figure()\n",
    "    # affichage du chiffre\n",
    "    # le paramètre interpolation='nearest' force python à afficher chaque valeur de la matrice sans l'interpoler avec ses voisines\n",
    "    # le paramètre cmap définit l'échelle de couleur utilisée (ici noire et blanc)\n",
    "    plt.imshow(image,interpolation='nearest',cmap='binary')\n",
    "    # on met un titre\n",
    "    plt.title('chiffre '+str(label))\n",
    "    # on affichage les figures créées\n",
    "    plt.show()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 1 : Perceptron "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### cette fonction permet de créer un reseau de neurones avec une couche d'entrée et une couche de sortie, elle a comme parametres les donnees d'entrés (image) et la variable cible (label) , ainsi que les poids initiaux du reseaux (avec la valeur constante du Biais) , le pas d'apprentissage et le nombre de neurone de la couche de sortie , qui correspond bien evidamment au nombre du classe du variable cible.\n",
    "\n",
    "###### le processus de cette methode consiste principalement sur le calcul de l'activité pour chaque neurone du couche de sortie, puis on mesure l'erreur de prediction afin de faire la mise à jour des poids pour adpater les valeurs predites au valeurs réelles des classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Single_Layer_Perceptron (image,label,weights,eta,output_nodes):\n",
    "   \n",
    "    x = image.numpy()[0]\n",
    "    \n",
    "    \n",
    "    for i in range(0,output_nodes):\n",
    "        activite = dot(x,weights[i][1:])+weights[i][0]  # weights[i][0]  : Biais\n",
    "        y = label.numpy()[0][i]\n",
    "        #print(\"y = \",y)\n",
    "        error = y - activite\n",
    "        #print(\"error = \",error)\n",
    "        weights[i][1:] += eta * error * x\n",
    "        weights[i][0] += error*eta \n",
    "        \n",
    "    \n",
    "    return weights\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### La phase d'apprentissage consiste à faire tourner ce processus sur plusieurs iterations afin de garantir la convergence des poids pour une meilleure classification.\n",
    "##### après plusieurs essais sur les differents paramétres du notre reseau Perceptron, on a remarqué que pour le perceptron simple avec une seule couche, les parametres qui donnent une meilleure precision au niveau de classification correspond principalement sur une valeur de pas d'apprentissge très petite , en plus le nombre d'iteration pour l'apprentissage peut influencer aussi sur les poids de notre perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = 28*28\n",
    "output_nodes = 10\n",
    "eta = 0.0001\n",
    "\n",
    "size = inputs*output_nodes\n",
    "w = Weights(size,-0.25,0.25)\n",
    "w = numpy.array(w).reshape(output_nodes, inputs)\n",
    "w = numpy.insert(w, 0, 1, axis=1) # add Biais\n",
    "\n",
    "nb_iteration = 100\n",
    "\n",
    "for i in range(0,nb_iteration):\n",
    "    for image,label in train_loader:\n",
    "        w = Single_Layer_Perceptron(image,label,w,eta,output_nodes)\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    \n",
    "for image in test_loader:\n",
    "    \n",
    "    x = image.numpy()[0]\n",
    "    List_predictions = []\n",
    "    for i in range(0,output_nodes):\n",
    "        \n",
    "        predicted = dot(x,w[i][1:])+w[i][0]\n",
    "        List_predictions.append(predicted)\n",
    "        \n",
    "    y_pred = numpy.argmax(List_predictions)\n",
    "    \n",
    "    affichage2(image.numpy(),y_pred)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  =  0.8072857142857143\n"
     ]
    }
   ],
   "source": [
    "List_predictions = []\n",
    "list_trueValues = []\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for image in test_loader:\n",
    "    \n",
    "    x = image.numpy()[0]\n",
    "\n",
    "    pred = []\n",
    "    #print(x)\n",
    "    for i in range(0,output_nodes):\n",
    "\n",
    "        predicted = dot(x,w[i][1:])\n",
    "        pred.append(predicted)\n",
    "    \n",
    "    y_pred = numpy.argmax(pred)\n",
    "    #print(pred)\n",
    "    list_trueValues.append(numpy.argmax(test_data_label[counter]))\n",
    "    List_predictions.append(y_pred)\n",
    "    counter = counter + 1\n",
    "    if (counter == len(test_data_label)):\n",
    "        break\n",
    "        \n",
    "    #print(\"predicted = \",y_pred , \"  true = \",numpy.argmax(test_data_label[o]))\n",
    "    \n",
    "p = 0\n",
    "\n",
    "# get values that were predicted right\n",
    "\n",
    "for b in range(0,len(List_predictions)):\n",
    "    if List_predictions[b] == list_trueValues[b]:\n",
    "        p = p + 1\n",
    "\n",
    "print(\"Accuracy  = \",p/7000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 2 : Shallow network\n",
    "### Multi Layer Perceptron "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Pour la definition du perceptron multi couche, la fonction prend comme paramètres bien evidamment les données d'entrée et la variable cible, plus les poids de chacune des deux couches du perceptron : couche cachée et la couche de sortie , et le nombre de neurones de la couche cachée et la couche de sortie\n",
    "\n",
    "##### On commence le procesus par presenter les donnees d'entrées pour effectuer apres la propagation de l'activité d'abord au niveau de la couche cachée qui va servir par la suite pour calculer l'activité du la couche de sortie. En revanche , l'étape de retro-propagation est faite pour calculer l'erreur pour les deux couches de perceptron, afin de faire la mise à jour de leurs poids en se basant sur l'erreur calculé du couche courante et sa donnée d'entrée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Multi_Layer_Perceptron (image,label,Hidden_Layer_weights,output_Layer_weights,eta,hidden_nodes,output_nodes):\n",
    "    \n",
    "    x = image.numpy()[0]\n",
    "    output_nodes_errors = []\n",
    "    \n",
    "    output_nodes_sigmoidValues = []\n",
    "    for k in range(0,output_nodes):\n",
    "        \n",
    "        y = label.numpy()[0][k]\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        hidden_sigmoid_output = []\n",
    "        \n",
    "        for i in range(0,hidden_nodes):\n",
    "            \n",
    "            hidden_activite = dot(x,Hidden_Layer_weights[i][1:])+Hidden_Layer_weights[i][0]\n",
    "            sig = sigmoid(hidden_activite)\n",
    "            hidden_sigmoid_output.append(sig)\n",
    "        \n",
    "        output_nodes_sigmoidValues.append(hidden_sigmoid_output)\n",
    "        \n",
    "        output_activite = dot(hidden_sigmoid_output,output_Layer_weights[k][1:])+output_Layer_weights[k][0]\n",
    "        output_error = y - output_activite\n",
    "        output_nodes_errors.append(output_error)\n",
    "        \n",
    "    hidden_nodes_errors = []\n",
    "    \n",
    "    w22 = numpy.transpose(output_Layer_weights)\n",
    "    w22 = numpy.delete(w22, 0, 0)\n",
    "    \n",
    "    allsigmo = numpy.transpose(output_nodes_sigmoidValues)\n",
    "\n",
    "    for i in range(0,hidden_nodes):\n",
    "        hidden_error = 0.0\n",
    "        for j in range(0,output_nodes):\n",
    "            \n",
    "            hidden_error += allsigmo[i][j]*(1-allsigmo[i][j])*output_nodes_errors[j]*w22[i][j]\n",
    "            \n",
    "        hidden_nodes_errors.append(hidden_error)\n",
    "        \n",
    "        \n",
    "        Hidden_Layer_weights[i][1:] += eta * hidden_error*x\n",
    "        Hidden_Layer_weights[i][0] += hidden_error*eta\n",
    "        \n",
    "      \n",
    "    for o in range(0,output_nodes):\n",
    "        \n",
    "        output_Layer_weights[o][1:] += eta * output_nodes_errors[o] * numpy.array(output_nodes_sigmoidValues[o])\n",
    "        output_Layer_weights[o][0] += output_nodes_errors[o]*eta\n",
    "                \n",
    "            \n",
    "\n",
    "             \n",
    "            \n",
    "    return Hidden_Layer_weights , output_Layer_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pour l'apprentissage , on remarque que toujours l'importance des parametres qui influencent concretement au qualité de classification , meme si on laisse le pas d'apprentissage pas assez petit qui peut causer une convergence rapide des poids mais par ailleurs l'augmentation du nombre des neurones du la couche cachée plus le nombre d'iterations pour l'apprentissage servent au fur et  à mesure de donner une precision plus importante au niveau du classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputs = 28*28\n",
    "hidden_nodes = 60\n",
    "output_nodes = 10\n",
    "eta = 0.02\n",
    "\n",
    "\n",
    "size1 = inputs*hidden_nodes\n",
    "size2 = hidden_nodes*output_nodes\n",
    "\n",
    "Hidden_Layer_weights = Weights(size1,-1,1)\n",
    "Hidden_Layer_weights = numpy.array(Hidden_Layer_weights).reshape(hidden_nodes, inputs)\n",
    "Hidden_Layer_weights = numpy.insert(Hidden_Layer_weights, 0, 1, axis=1)\n",
    "\n",
    "output_Layer_weights = Weights(size2,-1,1)\n",
    "output_Layer_weights = numpy.array(output_Layer_weights).reshape(output_nodes, hidden_nodes)\n",
    "output_Layer_weights = numpy.insert(output_Layer_weights, 0, 1, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "#print(\"intial weights = \",w2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0,10):\n",
    "    for image,label in train_loader:\n",
    "\n",
    "        #affichage(image.numpy(),label.numpy())\n",
    "        Hidden_Layer_weights , output_Layer_weights = Multi_Layer_Perceptron(image,label,Hidden_Layer_weights,output_Layer_weights,eta,hidden_nodes,output_nodes)  \n",
    "\n",
    "\n",
    "#print(\"final weights = \",w1,w2)        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  =  0.9188571428571428\n"
     ]
    }
   ],
   "source": [
    "List_predictions = []\n",
    "list_trueValues = []\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for image in test_loader:\n",
    "    \n",
    "    x = image.numpy()[0]\n",
    "\n",
    "    pred = []\n",
    "    #print(x)\n",
    "    for i in range(0,output_nodes):\n",
    "        sigmo = []\n",
    "        for j in range(0,hidden_nodes):\n",
    "            activite1 = dot(x,Hidden_Layer_weights[j][1:])+Hidden_Layer_weights[j][0]\n",
    "            sig = sigmoid(activite1)\n",
    "            sigmo.append(sig)\n",
    "        \n",
    "        activite2 = dot(sigmo,output_Layer_weights[i][1:])+output_Layer_weights[i][0]\n",
    "        pred.append(activite2)    \n",
    "    \n",
    "    y_pred = numpy.argmax(pred)\n",
    "    #print(pred)\n",
    "    list_trueValues.append(numpy.argmax(test_data_label[counter]))\n",
    "    List_predictions.append(y_pred)\n",
    "    counter = counter + 1\n",
    "    if (counter == len(test_data_label)):\n",
    "        break\n",
    "    #affichage2(image.numpy(),y_pred)\n",
    "    #print(\"predicted = \",y_pred , \"  true = \",numpy.argmax(test_data_label[o]))\n",
    "p = 0\n",
    "\n",
    "for b in range(0,len(list_trueValues)):\n",
    "    if List_predictions[b] == list_trueValues[b]:\n",
    "        p = p + 1\n",
    "\n",
    "print(\"Accuracy  = \",p/7000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## partie 3 : Deep Network\n",
    "### Model and training phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Le principe du definition du model torch consite à creer les differents couches du model (les couches cachéeset la couche de sortie) , alors la focntion prend comme parametres les donnees d'entrées , le nombre des couches cachées que vous voulez crée , plus une liste (hidden_layers_nodes) qui contient le nobre des neurones pour chaque couche cachée .\n",
    "##### Dans ce cas là, on a utilisé lq fonction sigmoid vue le meilleur resultat qui donne par rapport aux autres focntions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def torch_model(input_nodes,number_hidden_layers,hidden_layers_nodes , output_nodes):\n",
    "    \n",
    "    layers = []\n",
    "    layers.append(torch.nn.Linear(input_nodes, hidden_layers_nodes[0]))\n",
    "    layers.append(torch.nn.Sigmoid())\n",
    "    for i in range(0,number_hidden_layers):\n",
    "\n",
    "        layers.append(torch.nn.Linear(hidden_layers_nodes[i], hidden_layers_nodes[i+1]))\n",
    "        layers.append(torch.nn.Sigmoid())\n",
    "    layers.append(torch.nn.Linear(hidden_layers_nodes[len(hidden_layers_nodes)-1],output_nodes))  \n",
    "    model = torch.nn.Sequential(*layers)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=200)\n",
       "  (1): Sigmoid()\n",
       "  (2): Linear(in_features=200, out_features=100)\n",
       "  (3): Sigmoid()\n",
       "  (4): Linear(in_features=100, out_features=50)\n",
       "  (5): Sigmoid()\n",
       "  (6): Linear(in_features=50, out_features=10)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch_model(28*28,2,[200,100,50],10)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### on remarque que plus le nombre des couches cachées est grand ainsi que le nombre de neurones de chacune de ces couches est aussi grand , la précision du classification s'améliore en corrélation\n",
    "##### aussi le nombre d'iteration pour l'apprentissage influence d'une manière importante à la precision  , meme si le pas d'apprentissage n'est pas petit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "Xtr=Variable(torch.stack([torch.Tensor(i) for i in train_data]))\n",
    "Ytr=Variable(torch.stack([torch.Tensor(i) for i in train_data_label]))\n",
    "\n",
    "Xte=Variable(torch.stack([torch.Tensor(i) for i in test_data]),requires_grad=False)\n",
    "Yte=Variable(torch.stack([torch.Tensor(i) for i in test_data_label]),requires_grad=False)\n",
    "\n",
    "learning_rate = 0.01\n",
    "# Use the nn package to define our model and loss function.\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(size_average=False)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "for t in range(500):\n",
    "    \n",
    "  # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = model(Xtr)\n",
    "\n",
    "  # Compute and print loss.\n",
    "    loss = loss_fn(y_pred, Ytr)\n",
    "    \n",
    "    #print(y_pred)\n",
    "  \n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Test phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = model(Xte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02299722,  0.00442303,  0.00881158,  0.0063893 , -0.01643628,\n",
       "        0.00310716, -0.00696279, -0.0080597 ,  0.00467176,  1.02625406], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr = numpy.array(pred.data)\n",
    "yt = numpy.array(Yte.data)\n",
    "pr[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  =  0.9728571428571429\n"
     ]
    }
   ],
   "source": [
    "cn = 0 \n",
    "for i in range(0,7000):\n",
    "    \n",
    "    pre = numpy.argmax(pr[i])\n",
    "    \n",
    "    ytrue = numpy.argmax(yt[i])\n",
    "    \n",
    "    if (pre == ytrue):\n",
    "        cn = cn + 1\n",
    "        \n",
    "print(\"Accuracy  = \",cn/7000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 4 : Optionnel\n",
    "\n",
    "### 1- Activation function\n",
    "\n",
    "#### Tanh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def torch_model(input_nodes,number_hidden_layers,hidden_layers_nodes , output_nodes):\n",
    "    \n",
    "    layers = []\n",
    "    layers.append(torch.nn.Linear(input_nodes, hidden_layers_nodes[0]))\n",
    "    layers.append(torch.nn.Tanh())\n",
    "    for i in range(0,number_hidden_layers):\n",
    "\n",
    "        layers.append(torch.nn.Linear(hidden_layers_nodes[i], hidden_layers_nodes[i+1]))\n",
    "        layers.append(torch.nn.Tanh())\n",
    "    layers.append(torch.nn.Linear(hidden_layers_nodes[len(hidden_layers_nodes)-1],output_nodes))  \n",
    "    model = torch.nn.Sequential(*layers)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=100)\n",
       "  (1): Tanh()\n",
       "  (2): Linear(in_features=100, out_features=80)\n",
       "  (3): Tanh()\n",
       "  (4): Linear(in_features=80, out_features=40)\n",
       "  (5): Tanh()\n",
       "  (6): Linear(in_features=40, out_features=10)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch_model(28*28,2,[100,80,40],10)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "Xtr=Variable(torch.stack([torch.Tensor(i) for i in train_data]))\n",
    "Ytr=Variable(torch.stack([torch.Tensor(i) for i in train_data_label]))\n",
    "\n",
    "Xte=Variable(torch.stack([torch.Tensor(i) for i in test_data]),requires_grad=False)\n",
    "Yte=Variable(torch.stack([torch.Tensor(i) for i in test_data_label]),requires_grad=False)\n",
    "\n",
    "learning_rate = 0.01\n",
    "# Use the nn package to define our model and loss function.\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(size_average=False)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "for t in range(500):\n",
    "    \n",
    "  # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = model(Xtr)\n",
    "\n",
    "  # Compute and print loss.\n",
    "    loss = loss_fn(y_pred, Ytr)\n",
    "    \n",
    "    #print(y_pred)\n",
    "  \n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  =  0.9717142857142858\n"
     ]
    }
   ],
   "source": [
    "pred = model(Xte)\n",
    "pr = numpy.array(pred.data)\n",
    "yt = numpy.array(Yte.data)\n",
    "cn = 0 \n",
    "for i in range(0,7000):\n",
    "    \n",
    "    pre = numpy.argmax(pr[i])\n",
    "    \n",
    "    ytrue = numpy.argmax(yt[i])\n",
    "    if (pre == ytrue):\n",
    "        cn = cn + 1\n",
    "        \n",
    "print(\"Accuracy  = \",cn/7000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReLU()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition and Traning phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def torch_model(input_nodes,number_hidden_layers,hidden_layers_nodes , output_nodes):\n",
    "    \n",
    "    layers = []\n",
    "    layers.append(torch.nn.Linear(input_nodes, hidden_layers_nodes[0]))\n",
    "    layers.append(torch.nn.ReLU())\n",
    "    for i in range(0,number_hidden_layers):\n",
    "\n",
    "        layers.append(torch.nn.Linear(hidden_layers_nodes[i], hidden_layers_nodes[i+1]))\n",
    "        layers.append(torch.nn.ReLU())\n",
    "    layers.append(torch.nn.Linear(hidden_layers_nodes[len(hidden_layers_nodes)-1],output_nodes))  \n",
    "    model = torch.nn.Sequential(*layers)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = torch_model(28*28,2,[200,100,50],10)\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "Xtr=Variable(torch.stack([torch.Tensor(i) for i in train_data]))\n",
    "Ytr=Variable(torch.stack([torch.Tensor(i) for i in train_data_label]))\n",
    "\n",
    "Xte=Variable(torch.stack([torch.Tensor(i) for i in test_data]),requires_grad=False)\n",
    "Yte=Variable(torch.stack([torch.Tensor(i) for i in test_data_label]),requires_grad=False)\n",
    "\n",
    "learning_rate = 0.01\n",
    "# Use the nn package to define our model and loss function.\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(size_average=False)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "for t in range(200):\n",
    "    \n",
    "  # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = model(Xtr)\n",
    "\n",
    "  # Compute and print loss.\n",
    "    loss = loss_fn(y_pred, Ytr)\n",
    "    \n",
    "    #print(y_pred)\n",
    "  \n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  =  0.968\n"
     ]
    }
   ],
   "source": [
    "pred = model(Xte)\n",
    "pr = numpy.array(pred.data)\n",
    "yt = numpy.array(Yte.data)\n",
    "cn = 0 \n",
    "for i in range(0,7000):\n",
    "    \n",
    "    pre = numpy.argmax(pr[i])\n",
    "    \n",
    "    ytrue = numpy.argmax(yt[i])\n",
    "    \n",
    "    if (pre == ytrue):\n",
    "        cn = cn + 1\n",
    "        \n",
    "print(\"Accuracy  = \",cn/7000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Optimizer\n",
    "\n",
    "#### Adagrad "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition and training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def torch_model(input_nodes,number_hidden_layers,hidden_layers_nodes , output_nodes):\n",
    "    \n",
    "    layers = []\n",
    "    layers.append(torch.nn.Linear(input_nodes, hidden_layers_nodes[0]))\n",
    "    layers.append(torch.nn.ReLU())\n",
    "    for i in range(0,number_hidden_layers):\n",
    "\n",
    "        layers.append(torch.nn.Linear(hidden_layers_nodes[i], hidden_layers_nodes[i+1]))\n",
    "        layers.append(torch.nn.ReLU())\n",
    "    layers.append(torch.nn.Linear(hidden_layers_nodes[len(hidden_layers_nodes)-1],output_nodes))  \n",
    "    model = torch.nn.Sequential(*layers)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = torch_model(28*28,2,[100,60,20],10)\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "Xtr=Variable(torch.stack([torch.Tensor(i) for i in train_data]))\n",
    "Ytr=Variable(torch.stack([torch.Tensor(i) for i in train_data_label]))\n",
    "\n",
    "Xte=Variable(torch.stack([torch.Tensor(i) for i in test_data]),requires_grad=False)\n",
    "Yte=Variable(torch.stack([torch.Tensor(i) for i in test_data_label]),requires_grad=False)\n",
    "\n",
    "learning_rate = 0.01\n",
    "# Use the nn package to define our model and loss function.\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(size_average=False)\n",
    "\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "for t in range(300):\n",
    "    \n",
    "  # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = model(Xtr)\n",
    "\n",
    "  # Compute and print loss.\n",
    "    loss = loss_fn(y_pred, Ytr)\n",
    "    \n",
    "    #print(y_pred)\n",
    "  \n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  =  0.9568571428571429\n"
     ]
    }
   ],
   "source": [
    "pred = model(Xte)\n",
    "pr = numpy.array(pred.data)\n",
    "yt = numpy.array(Yte.data)\n",
    "cn = 0 \n",
    "for i in range(0,7000):\n",
    "    \n",
    "    pre = numpy.argmax(pr[i])\n",
    "    \n",
    "    ytrue = numpy.argmax(yt[i])\n",
    "    \n",
    "    if (pre == ytrue):\n",
    "        cn = cn + 1\n",
    "        \n",
    "print(\"Accuracy  = \",cn/7000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3- Convolutionnal Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 4D tensor as input, got 3D tensor instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-0feac490bbd8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     37\u001b[0m   \u001b[1;31m# Forward pass: compute predicted y by passing x to the model.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mXtr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mXtr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m   \u001b[1;31m# Compute and print loss.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\PC\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\PC\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\PC\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\PC\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    275\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[1;32m--> 277\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\PC\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(input, weight, bias, stride, padding, dilation, groups)\u001b[0m\n\u001b[0;32m     83\u001b[0m     \"\"\"\n\u001b[0;32m     84\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Expected 4D tensor as input, got {}D tensor instead.\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m     f = _ConvNd(_pair(stride), _pair(padding), _pair(dilation), False,\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 4D tensor as input, got 3D tensor instead."
     ]
    }
   ],
   "source": [
    "def torch_model(input_nodes,number_hidden_layers,hidden_layers_nodes , output_nodes):\n",
    "    \n",
    "    layers = []\n",
    "    layers.append(torch.nn.Conv2d(input_nodes, hidden_layers_nodes[0],2))\n",
    "    layers.append(torch.nn.ReLU())\n",
    "    for i in range(0,number_hidden_layers):\n",
    "\n",
    "        layers.append(torch.nn.Conv2d(hidden_layers_nodes[i], hidden_layers_nodes[i+1],2))\n",
    "        layers.append(torch.nn.ReLU())\n",
    "    layers.append(torch.nn.Conv2d(hidden_layers_nodes[len(hidden_layers_nodes)-1],output_nodes,2))  \n",
    "    model = torch.nn.Sequential(*layers)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = torch_model(28*28,2,[80,60,20],10)\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "Xtr=Variable(torch.stack([torch.Tensor(i) for i in train_data]))\n",
    "Ytr=Variable(torch.stack([torch.Tensor(i) for i in train_data_label]))\n",
    "\n",
    "Xte=Variable(torch.stack([torch.Tensor(i) for i in test_data]),requires_grad=False)\n",
    "Yte=Variable(torch.stack([torch.Tensor(i) for i in test_data_label]),requires_grad=False)\n",
    "\n",
    "learning_rate = 0.01\n",
    "# Use the nn package to define our model and loss function.\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(size_average=False)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "for t in range(200):\n",
    "    \n",
    "  # Forward pass: compute predicted y by passing x to the model.\n",
    "    Xtr = Xtr.unsqueeze(0)\n",
    "    y_pred = model(Xtr)\n",
    "\n",
    "  # Compute and print loss.\n",
    "    loss = loss_fn(y_pred, Ytr)\n",
    "    \n",
    "    #print(y_pred)\n",
    "  \n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.nn.Conv2d?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
